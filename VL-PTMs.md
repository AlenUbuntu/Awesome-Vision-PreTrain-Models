# VL-PTMs
## Table of Contents
* [Image-Based VL-PTMs](#image-based-vl-ptms)
  * [Representation Learning](#representation-learning)
  * [Task-Specific](#task-specific)
  * [Others](#others)
* [Video-Based VL-PTMs](#video-based-vl-ptms)

## Image-Based VL-PTMs
### Representation Learning
**MAG-BERT, MAG-XLNet**

[Integrating Multimodal Information in Large Pretrained Transformers.](https://www.aclweb.org/anthology/2020.acl-main.214.pdf), ACL, 2020.

**X-LXMERT**

[X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf), EMNLP, 2020.

**VL-BERT**

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/pdf/1908.08530.pdf), ICLR, 2020.

**Unicoder-VL**

[Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training](https://arxiv.org/pdf/1908.06066.pdf), AAAI, 2020.

**VLP**

[Unified Vision-Language Pre-Training for Image Captioning and VQA](https://arxiv.org/pdf/1909.11059.pdf), AAAI, 2020.

**InterBERT**

[InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction](https://arxiv.org/pdf/2003.13198.pdf), arXiv, 2020.

**Oscar**

[Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/pdf/2004.06165.pdf), arXiv, 2020.

**Pixel-BERT**

[Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers](https://arxiv.org/pdf/2004.00849.pdf), arXiv, 2020.

**ERNIE-ViL**

[ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph](https://arxiv.org/pdf/2006.16934.pdf), arXiv, 2020.

**DeVLBert**

[DeVLBert: Learning Deconfounded Visio-Linguistic Representations](https://arxiv.org/pdf/2008.06884.pdf), ACM MM, 2020.

**SEMVLP**

[SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels](https://openreview.net/pdf?id=Wg2PSpLZiH), ICLR, 2021.

**ViLBERT**

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/pdf/1908.02265.pdf), NIPS, 2019

**LXMERT**

[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/pdf/1908.07490.pdf), EMNLP, 2019.

**VisualBERT**

[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557.pdf), arXiv, 2019.

**UNITER**

[UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/pdf/1909.11740.pdf), arXiv, 2019.

**Vision-Language Encoder**

[Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks](https://arxiv.org/pdf/1912.03063.pdf), arXiv, 2019. 

### Task-Specific
**Image Caption**: [Meshed-Memory Transformer for Image Captioning](https://arxiv.org/pdf/1912.08226.pdf), CVPR, 2020.

**Image Caption**: [XGPT: Cross-modal Generative Pre-Training for Image Captioning](https://arxiv.org/pdf/2003.01473.pdf), arXiv, 2020.

**Image Caption**: [Entangled Transformer for Image Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf), ICCV, 2019.

**Machine Translation**: [Multimodal Transformer for Multimodal Machine Translation.](https://www.aclweb.org/anthology/2020.acl-main.400.pdf), ACL, 2020.

**Classification**: [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf), arXiv, 2019

**VQA**: [Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA](https://arxiv.org/pdf/1911.06258.pdf), CVPR, 2020.

**VQA**: [Spatially Aware Multimodal Transformers for TextVQA](https://arxiv.org/pdf/2007.12146.pdf), ECCV, 2020.

**NER**: [Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer](https://www.aclweb.org/anthology/2020.acl-main.306.pdf), ACL, 2020.

**VisDial**: [VD-BERT: A Unified Vision and Dialog Transformer with BERT](https://arxiv.org/pdf/2004.13278.pdf), EMNLP, 2020.

**VisDial**: [Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline](https://arxiv.org/pdf/1912.02379.pdf), ECCV, 2020.

**Vision-and-Language Navigation**: [Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training](https://arxiv.org/pdf/2002.10638.pdf), CVPR, 2020.

**Text-Image Retrieval**: [ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data](https://arxiv.org/pdf/2001.07966.pdf), arXiv, 2020.

**Text-Image Retrieval**: [Cross-Probe BERT for Efficient and Effective Cross-Modal Search](https://openreview.net/pdf?id=bW9SYKHcZiz), ICLR, 2021.

**Visial Common Sense Reasoning**: [Fusion of Detected Objects in Text for Visual Question Answering](https://arxiv.org/pdf/1908.05054.pdf), EMNLP, 2019.


### Others
[Are we pretraining it right? Digging deeper into visio-linguistic pretraining](https://arxiv.org/pdf/2004.08744.pdf), arXiv, 2020.

[Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models](https://arxiv.org/pdf/2005.07310.pdf), arXiv, 2020.

[12-in-1: Multi-Task Vision and Language Representation Learning](https://arxiv.org/pdf/1912.02315.pdf), arXiv, 2020.

## Video-Based VL-PTMs
### Representation Learning
**COOT**

[COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning](https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf), NIPS, 2020.

**MulT**

[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://www.aclweb.org/anthology/P19-1656.pdf), ACL, 2019.

### Task-Specific

**Action Segmentation**: [SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation](https://arxiv.org/pdf/2003.14266.pdf), CVPR, 2020.

**Video Retrieval**: [Multi-modal Transformer for Video Retrieval](https://arxiv.org/pdf/2007.10639.pdf), ECCV, 2020.

**VQA**: [Video-Grounded Dialogues with Pretrained Generation Language Models.](https://www.aclweb.org/anthology/2020.acl-main.518.pdf), ACL, 2020.

**VQA**: [Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems](https://www.aclweb.org/anthology/P19-1564.pdf), ACL, 2019.

