## Language-Only PTMs
### Modeling Techniques
[To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks.](https://www.aclweb.org/anthology/2020.acl-main.200.pdf), ACL, 2020.

[Quantifying Attention Flow in Transformers](https://www.aclweb.org/anthology/2020.acl-main.385.pdf), ACL, 2020.

[Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture](https://www.aclweb.org/anthology/2020.acl-main.360.pdf), ACL, 2020.

[DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering](https://www.aclweb.org/anthology/2020.acl-main.411.pdf), ACL, 2020.

[Byte Pair Encoding is Suboptimal for Language Model Pretraining.](https://www.aclweb.org/anthology/2020.findings-emnlp.414.pdf) EMNLP(Findings), 2020.

[Analyzing Redundancy in Pretrained Transformer Models.](https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf), EMNLP, 2020.

[How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://www.aclweb.org/anthology/2020.findings-emnlp.394.pdf), EMNLP(Findings), 2020.

[Pretrained Language Model Embryology: The Birth of ALBERT.](https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf), EMNLP, 2020.

[Pre-Training Transformers as Energy-Based Cloze Models.](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf), EMNLP, 2020.

[Calibration of Pre-trained Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.21.pdf), EMNLP, 2020.

[Guiding Attention for Self-Supervised Learning with Transformers](https://www.aclweb.org/anthology/2020.findings-emnlp.419.pdf), EMNLP(Findings), 2020.

[Improve Transformer Models with Better Relative Position Embeddings](https://www.aclweb.org/anthology/2020.findings-emnlp.298.pdf), EMNLP(Findings), 2020.

[Long Document Ranking with Query-Directed Sparse Transformer](https://www.aclweb.org/anthology/2020.findings-emnlp.412.pdf), EMNLP(Findings), 2020.

[Attention is Not Only a Weight: Analyzing Transformers with Vector Norms](https://www.aclweb.org/anthology/2020.emnlp-main.574.pdf), EMNLP, 2020.

[Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior](https://www.aclweb.org/anthology/2020.findings-emnlp.64.pdf), EMNLP(Findings), 2020.

[Understanding the Difficulty of Training Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf), EMNLP, 2020.

[AdapterHub: A Framework for Adapting Transformers](https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf), EMNLP(Demo), 2020.

[Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings](https://www.aclweb.org/anthology/2020.findings-emnlp.423.pdf), EMNLP(Findings), 2020.

[On the Sub-layer Functionalities of Transformer Decoder](https://www.aclweb.org/anthology/2020.findings-emnlp.432.pdf), EMNLP(Findings), 2020.

[Scheduled DropHead: A Regularization Method for Transformer Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.178.pdf), EMNLP(Findings), 2020.

[Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling.](https://www.aclweb.org/anthology/P19-1439.pdf), ACL, 2019.

### Transfer Learning
[Unsupervised Domain Clusters in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.692.pdf), ACL, 2020.

[Emerging Cross-lingual Structure in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.536.pdf) ACL, 2020.

[Don't Stop Pretraining: Adapt Language Models to Domains and Tasks.](https://www.aclweb.org/anthology/2020.acl-main.740.pdf), ACL, 2020.

[Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://www.aclweb.org/anthology/2020.acl-main.467.pdf), ACL, 2020.

[X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf), EMNLP, 2020.

[A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf), EMNLP, 2020.

[Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting.](https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf), EMNLP, 2020.

[Investigating Transferability in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.125/), EMNLP(Findings), 2020.

[Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning.](https://www.aclweb.org/anthology/2020.findings-emnlp.285.pdf), EMNLP(Findings), 2020.

[Masking as an Efficient Alternative to Finetuning for Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf), EMNLP, 2020.

[Factorized Transformer for Multi-Domain Neural Machine Translation](https://www.aclweb.org/anthology/2020.findings-emnlp.377.pdf), EMNLP, 2020.

[From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf), EMNLP, 2020.

[A Bilingual Generative Transformer for Semantic Sentence Embedding](https://www.aclweb.org/anthology/2020.emnlp-main.122.pdf), EMNLP, 2020.

[Transformer Based Multi-Source Domain Adaptation](https://www.aclweb.org/anthology/2020.emnlp-main.639.pdf), EMNLP, 2020.

[KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations](https://www.aclweb.org/anthology/2020.emnlp-main.18.pdf), EMNLP, 2020.


[A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings.](https://ojs.aaai.org//index.php/AAAI/article/view/6443), AAAI, 2020.

[Cross-lingual Language Model Pretraining](https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf), NIPS, 2019.

### Multi-Modal
[Integrating Multimodal Information in Large Pretrained Transformers.](https://www.aclweb.org/anthology/2020.acl-main.214.pdf), ACL, 2020.

[TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data.](https://www.aclweb.org/anthology/2020.acl-main.745.pdf), ACL, 2020.

[X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf), EMNLP, 2020.

### Others
[Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media.](https://www.aclweb.org/anthology/2020.findings-emnlp.151.pdf), EMNLP(Findings), 2020.

[Multi-pretraining for Large-scale Text Classification.](https://www.aclweb.org/anthology/2020.findings-emnlp.185.pdf), EMNLP(Findings), 2020.

[BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA.](https://www.aclweb.org/anthology/2020.findings-emnlp.307.pdf), EMNLP, 2020.

[Document Ranking with a Pretrained Sequence-to-Sequence Model.](https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf), EMNLP(Findings), 2020.

[Probing Pretrained Language Models for Lexical Semantics.](https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf), EMNLP, 2020.

[Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.25.pdf), EMNLP(Findings), 2020.

[Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models](https://www.aclweb.org/anthology/2020.emnlp-main.679.pdf), EMNLP, 2020

[Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning](https://www.aclweb.org/anthology/2020.findings-emnlp.286.pdf), EMNLP(Findings), 2020.

[A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media](https://www.aclweb.org/anthology/2020.emnlp-main.619.pdf), EMNLP, 2020.

[TNT: Text Normalization based Pre-training of Transformers for Content Moderation](https://www.aclweb.org/anthology/2020.emnlp-main.383.pdf), EMNLP, 2020.

[Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations](https://www.aclweb.org/anthology/2020.emnlp-main.108.pdf), EMNLP, 2020.

[Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation](https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf), EMNLP, 2020.

[Table Fact Verification with Structure-Aware Transformer](https://www.aclweb.org/anthology/2020.emnlp-main.126.pdf), EMNLP, 2020.

[Taming Pretrained Transformers for Extreme Multi-label Text Classification](https://arxiv.org/pdf/1905.02331.pdf), KDD, 2020.

[Reciptor: An Effective Pretrained Model for Recipe Representation Learning](https://dl.acm.org/doi/pdf/10.1145/3394486.3403223), KDD, 2020.

[Pretraining Methods for Dialog Context Representation Learning.](https://www.aclweb.org/anthology/P19-1373.pdf), ACL, 2019.

[Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings.](https://www.aclweb.org/anthology/P19-1168.pdf), ACL, 2019
