# Awesome Vision and Language PreTrain Models (PTMs)
Maintained by [Yang Gao]() (ustcgaoy01@gmail.com) Last Update on 11/20/2020.

## Table of Contents
* [Surveys](#survey)
* [Vision-Only PTMs](#vision-only-ptms)
* [Language-Only PTMs](#language-only-ptms)
* [Vision-Language PTMs](#vision-language-ptms)

## Survey
[Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)

## Vision-Only PTMs

## Language-Only PTMs
[Effectively pretraining a speech translation decoder with Machine Translation data](https://www.aclweb.org/anthology/2020.emnlp-main.644.pdf) EMNLP, 2020.

[Byte Pair Encoding is Suboptimal for Language Model Pretraining.](https://www.aclweb.org/anthology/2020.findings-emnlp.414.pdf) EMNLP(Findings), 2020.

[Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting.](https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf), EMNLP, 2020.

[Pretrained Language Model Embryology: The Birth of ALBERT.](https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf), EMNLP, 2020.

[Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT.](https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf) EMNLP, 2020.

[Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media.](https://www.aclweb.org/anthology/2020.findings-emnlp.151.pdf), EMNLP(Findings), 2020.

[Analyzing Redundancy in Pretrained Transformer Models.](https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf), EMNLP, 2020.

[DagoBERT: Generating Derivational Morphology with a Pretrained Language Model.](https://www.aclweb.org/anthology/2020.emnlp-main.316.pdf), EMNLP, 2020.

[X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf), EMNLP, 2020.

[BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA.](https://www.aclweb.org/anthology/2020.findings-emnlp.307.pdf), EMNLP, 2020.

[Multi-pretraining for Large-scale Text Classification.](https://www.aclweb.org/anthology/2020.findings-emnlp.185.pdf), EMNLP(Findings), 2020.

[A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf), EMNLP, 2020.

[How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://www.aclweb.org/anthology/2020.findings-emnlp.394.pdf), EMNLP(Findings), 2020.

[Document Ranking with a Pretrained Sequence-to-Sequence Model.](https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf), EMNLP(Findings), 2020.

[Investigating Transferability in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.125/), EMNLP(Findings), 2020.

[Probing Pretrained Language Models for Lexical Semantics.](https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf), EMNLP, 2020.

[Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning.](https://www.aclweb.org/anthology/2020.findings-emnlp.285.pdf), EMNLP(Findings), 2020.

[TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising.](https://www.aclweb.org/anthology/2020.findings-emnlp.168.pdf), EMNLP(Findings), 2020.

[Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.25.pdf), EMNLP(Findings), 2020.

[Masking as an Efficient Alternative to Finetuning for Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf), EMNLP, 2020.

[A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining](https://www.aclweb.org/anthology/2020.findings-emnlp.19.pdf), EMNLP(Findings), 2020.


## Vision-Language PTMs
### Image-Based VL-PTMs

### Video-Based VL-PTMs

### 3D VL-PTMs


