# Awesome Vision and Language PreTrain Models (PTMs)
Maintained by [Yang Gao]() (ustcgaoy01@gmail.com) Last Update on 11/20/2020. Due to the large amount of research in this field, we only focus on those researches related to modeling and representation learning, transfer learning, and multi-modal learning.

## Table of Contents
* [Surveys](#survey)
* [Vision-Only PTMs](#vision-only-ptms)
* [Language-Only PTMs](#language-only-ptms)
* [Vision-Language PTMs](#vision-language-ptms)

## Survey
[Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)

## Vision-Only PTMs

## Language-Only PTMs
### Modeling
[Integrating Multimodal Information in Large Pretrained Transformers.](https://www.aclweb.org/anthology/2020.acl-main.214.pdf), ACL, 2020.

[To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks.](https://www.aclweb.org/anthology/2020.acl-main.200.pdf), ACL, 2020.

[TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data.](https://www.aclweb.org/anthology/2020.acl-main.745.pdf), ACL, 2020.

[Byte Pair Encoding is Suboptimal for Language Model Pretraining.](https://www.aclweb.org/anthology/2020.findings-emnlp.414.pdf) EMNLP(Findings), 2020.

[Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media.](https://www.aclweb.org/anthology/2020.findings-emnlp.151.pdf), EMNLP(Findings), 2020.

[Analyzing Redundancy in Pretrained Transformer Models.](https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf), EMNLP, 2020.

[BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA.](https://www.aclweb.org/anthology/2020.findings-emnlp.307.pdf), EMNLP, 2020.

[Multi-pretraining for Large-scale Text Classification.](https://www.aclweb.org/anthology/2020.findings-emnlp.185.pdf), EMNLP(Findings), 2020.

[How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://www.aclweb.org/anthology/2020.findings-emnlp.394.pdf), EMNLP(Findings), 2020.

[Document Ranking with a Pretrained Sequence-to-Sequence Model.](https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf), EMNLP(Findings), 2020.

[Probing Pretrained Language Models for Lexical Semantics.](https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf), EMNLP, 2020.

[Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.25.pdf), EMNLP(Findings), 2020.

[Taming Pretrained Transformers for Extreme Multi-label Text Classification](https://arxiv.org/pdf/1905.02331.pdf), KDD, 2020.

[Reciptor: An Effective Pretrained Model for Recipe Representation Learning](https://dl.acm.org/doi/pdf/10.1145/3394486.3403223), KDD, 2020.

[Pretraining Methods for Dialog Context Representation Learning.](https://www.aclweb.org/anthology/P19-1373.pdf), ACL, 2019.

[Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings.](https://www.aclweb.org/anthology/P19-1168.pdf), ACL, 2019

[Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling.](https://www.aclweb.org/anthology/P19-1439.pdf), ACL, 2019.

[XLNet: Generalized Autoregressive Pretraining for Language Understanding.](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf), NIPS, 2019.

### Interpretability
[Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings.](https://www.aclweb.org/anthology/2020.acl-main.431.pdf), ACL, 2020.

[Pretrained Language Model Embryology: The Birth of ALBERT.](https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf), EMNLP, 2020.

### Language Generation
[Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models.](https://www.aclweb.org/anthology/2020.acl-main.439.pdf), ACL, 2020.

[Effectively pretraining a speech translation decoder with Machine Translation data](https://www.aclweb.org/anthology/2020.emnlp-main.644.pdf) EMNLP, 2020.

[Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT.](https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf) EMNLP, 2020.

[DagoBERT: Generating Derivational Morphology with a Pretrained Language Model.](https://www.aclweb.org/anthology/2020.emnlp-main.316.pdf), EMNLP, 2020.

[TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising.](https://www.aclweb.org/anthology/2020.findings-emnlp.168.pdf), EMNLP(Findings), 2020.

[A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining](https://www.aclweb.org/anthology/2020.findings-emnlp.19.pdf), EMNLP(Findings), 2020.

[Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation.](https://www.aclweb.org/anthology/P19-2017.pdf), ACL, 2019

### Transfer Learning
[Unsupervised Domain Clusters in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.692.pdf), ACL, 2020.

[Emerging Cross-lingual Structure in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.536.pdf) ACL, 2020.

[Don't Stop Pretraining: Adapt Language Models to Domains and Tasks.](https://www.aclweb.org/anthology/2020.acl-main.740.pdf), ACL, 2020.

[Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://www.aclweb.org/anthology/2020.acl-main.467.pdf), ACL, 2020.

[X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf), EMNLP, 2020.

[A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf), EMNLP, 2020.

[Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting.](https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf), EMNLP, 2020.

[Investigating Transferability in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.125/), EMNLP(Findings), 2020.

[Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning.](https://www.aclweb.org/anthology/2020.findings-emnlp.285.pdf), EMNLP(Findings), 2020.

[Masking as an Efficient Alternative to Finetuning for Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf), EMNLP, 2020.

[A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings.](https://ojs.aaai.org//index.php/AAAI/article/view/6443), AAAI, 2020.

[Cross-lingual Language Model Pretraining](https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf), NIPS, 2019.

### Robustness and Stability
[Pretrained Transformers Improve Out-of-Distribution Robustness.](https://www.aclweb.org/anthology/2020.acl-main.244.pdf), ACL, 2020.

[Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly.](https://www.aclweb.org/anthology/2020.acl-main.698.pdf), ACL, 2020.

[Weight Poisoning Attacks on Pretrained Models.](https://www.aclweb.org/anthology/2020.acl-main.249.pdf), ACL, 2020.

## Vision-Language PTMs
### Image-Based VL-PTMs

### Video-Based VL-PTMs
[Video-Grounded Dialogues with Pretrained Generation Language Models.](https://www.aclweb.org/anthology/2020.acl-main.518.pdf), ACL, 2020.



### 3D VL-PTMs


