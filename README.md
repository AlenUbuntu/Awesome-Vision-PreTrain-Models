# Awesome Vision and Language PreTrain Models (PTMs)
Maintained by [Yang Gao]() (ustcgaoy01@gmail.com) Last Update on 12/05/2020. 

Due to the large amount of research in this field, we mainly focus on research related to transfer learning, multimodal learning and SOTA modeling techniques for improving model performance or efficiency.

## Table of Contents
* [Surveys](#survey)
* [Transformers](#transformers)
* [Vision-Only PTMs](#vision-only-ptms)
* [Language-Only PTMs](#language-only-ptms)
* [Vision-Language PTMs](#vision-language-ptms)

## Survey
[Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)

[Transformers: State-of-the-Art Natural Language Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6.pdf)

## Transformers
![](https://github.com/AlenUbuntu/Awesome-Vision-and-Language-PreTrain-Models/blob/main/transformers.png)

**Performer**

[Masked language modeling for proteins via linearly scalable long-context transformers](https://arxiv.org/pdf/2006.03555.pdf), arXiv, 2020.

**Linformer**

[Linformer: Selfattention with linear complexity.](https://arxiv.org/pdf/2006.04768.pdf), arXiv, 2020.

**Linear Transformers**

[Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf), arXiv, 2020.

**BigBird**

[Big bird: Transformers for longer sequences.](https://arxiv.org/pdf/2007.14062.pdf), arXiv, 2020.

**Synthesizer**

[Synthesizer: Rethinking self-attention in transformer models.](https://arxiv.org/pdf/2005.00743.pdf), arXiv, 2020.

**ETC**

[Etc: Encoding long and structured data in transformers.](https://arxiv.org/pdf/2004.08483.pdf), arXiv, 2020.

**Longformer**

[Longformer: The long-document transformer.](https://arxiv.org/pdf/2004.05150.pdf), arXiv, 2020.

**Sinkhorn Transformer**

[Sparse sinkhorn attention.](https://arxiv.org/pdf/2002.11296.pdf), arXiv, 2020.

**Compressive Transformer**

[Compressive transformers for long-range sequence modelling.](https://arxiv.org/pdf/1911.05507.pdf), ICLR, 2020.

**Routing Transformer**

[Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/pdf/2003.05997.pdf), arXiv, 2020.

**Reformer**

[Reformer: The efficient transformer.](https://arxiv.org/pdf/2001.04451.pdf), ICLR, 2020.

**Axial Transformer**

[Axial attention in multidimensional transformers.](https://arxiv.org/pdf/1912.12180.pdf), arXiv, 2019.

**Sparse Transformer**

[Generating long sequences with sparse transformers.](https://arxiv.org/pdf/1904.10509.pdf)

**Transformer-XL**

[Transformer-xl: Attentive language models beyond a fixed-length context.](https://arxiv.org/pdf/1901.02860.pdf), arXiv, 2019.

**Set Transformer**

[Set transformer: A framework for attention-based permutation-invariant neural networks.](https://arxiv.org/pdf/1810.00825.pdf), ICML, 2019.

**Image Transformer**

[Image transformer.](https://arxiv.org/pdf/1802.05751.pdf), ICML, 2018.

**Memory Compressed**

[Generating wikipedia by summarizing long sequences](https://arxiv.org/pdf/1801.10198.pdf), ICLR, 2018.

**Transformer**

[Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf), NuerIPS, 2017

**Memory-driven Transformer**

[Generating Radiology Reports via Memory-driven Transformer](https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf), EMNLP, 2020.

## Vision-Only PTMs

## Language-Only PTMs
### Modeling Techniques
[To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks.](https://www.aclweb.org/anthology/2020.acl-main.200.pdf), ACL, 2020.

[Quantifying Attention Flow in Transformers](https://www.aclweb.org/anthology/2020.acl-main.385.pdf), ACL, 2020.

[Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture](https://www.aclweb.org/anthology/2020.acl-main.360.pdf), ACL, 2020.

[DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering](https://www.aclweb.org/anthology/2020.acl-main.411.pdf), ACL, 2020.

[Highway Transformer: Self-Gating Enhanced Self-Attentive Networks](https://www.aclweb.org/anthology/2020.acl-main.616.pdf), ACL, 2020.

[Byte Pair Encoding is Suboptimal for Language Model Pretraining.](https://www.aclweb.org/anthology/2020.findings-emnlp.414.pdf) EMNLP(Findings), 2020.

[Analyzing Redundancy in Pretrained Transformer Models.](https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf), EMNLP, 2020.

[How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://www.aclweb.org/anthology/2020.findings-emnlp.394.pdf), EMNLP(Findings), 2020.

[Pretrained Language Model Embryology: The Birth of ALBERT.](https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf), EMNLP, 2020.

[Pre-Training Transformers as Energy-Based Cloze Models.](https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf), EMNLP, 2020.

[Calibration of Pre-trained Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.21.pdf), EMNLP, 2020.

[Guiding Attention for Self-Supervised Learning with Transformers](https://www.aclweb.org/anthology/2020.findings-emnlp.419.pdf), EMNLP(Findings), 2020.

[Improve Transformer Models with Better Relative Position Embeddings](https://www.aclweb.org/anthology/2020.findings-emnlp.298.pdf), EMNLP(Findings), 2020.

[Long Document Ranking with Query-Directed Sparse Transformer](https://www.aclweb.org/anthology/2020.findings-emnlp.412.pdf), EMNLP(Findings), 2020.

[Attention is Not Only a Weight: Analyzing Transformers with Vector Norms](https://www.aclweb.org/anthology/2020.emnlp-main.574.pdf), EMNLP, 2020.

[Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior](https://www.aclweb.org/anthology/2020.findings-emnlp.64.pdf), EMNLP(Findings), 2020.

[Understanding the Difficulty of Training Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf), EMNLP, 2020.

[AdapterHub: A Framework for Adapting Transformers](https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf), EMNLP(Demo), 2020.

[Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings](https://www.aclweb.org/anthology/2020.findings-emnlp.423.pdf), EMNLP(Findings), 2020.

[On the Sub-layer Functionalities of Transformer Decoder](https://www.aclweb.org/anthology/2020.findings-emnlp.432.pdf), EMNLP(Findings), 2020.

[Scheduled DropHead: A Regularization Method for Transformer Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.178.pdf), EMNLP(Findings), 2020.

[Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling.](https://www.aclweb.org/anthology/P19-1439.pdf), ACL, 2019.

[XLNet: Generalized Autoregressive Pretraining for Language Understanding.](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf), NIPS, 2019.

### Transfer Learning
[Unsupervised Domain Clusters in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.692.pdf), ACL, 2020.

[Emerging Cross-lingual Structure in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.acl-main.536.pdf) ACL, 2020.

[Don't Stop Pretraining: Adapt Language Models to Domains and Tasks.](https://www.aclweb.org/anthology/2020.acl-main.740.pdf), ACL, 2020.

[Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://www.aclweb.org/anthology/2020.acl-main.467.pdf), ACL, 2020.

[X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf), EMNLP, 2020.

[A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf), EMNLP, 2020.

[Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting.](https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf), EMNLP, 2020.

[Investigating Transferability in Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.125/), EMNLP(Findings), 2020.

[Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning.](https://www.aclweb.org/anthology/2020.findings-emnlp.285.pdf), EMNLP(Findings), 2020.

[Masking as an Efficient Alternative to Finetuning for Pretrained Language Models.](https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf), EMNLP, 2020.

[Factorized Transformer for Multi-Domain Neural Machine Translation](https://www.aclweb.org/anthology/2020.findings-emnlp.377.pdf), EMNLP, 2020.

[From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf), EMNLP, 2020.

[A Bilingual Generative Transformer for Semantic Sentence Embedding](https://www.aclweb.org/anthology/2020.emnlp-main.122.pdf), EMNLP, 2020.

[Transformer Based Multi-Source Domain Adaptation](https://www.aclweb.org/anthology/2020.emnlp-main.639.pdf), EMNLP, 2020.

[KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations](https://www.aclweb.org/anthology/2020.emnlp-main.18.pdf), EMNLP, 2020.


[A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings.](https://ojs.aaai.org//index.php/AAAI/article/view/6443), AAAI, 2020.

[Cross-lingual Language Model Pretraining](https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf), NIPS, 2019.

### Multi-Modal
[Integrating Multimodal Information in Large Pretrained Transformers.](https://www.aclweb.org/anthology/2020.acl-main.214.pdf), ACL, 2020.

[TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data.](https://www.aclweb.org/anthology/2020.acl-main.745.pdf), ACL, 2020.

[X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf), EMNLP, 2020.

### Others
[Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media.](https://www.aclweb.org/anthology/2020.findings-emnlp.151.pdf), EMNLP(Findings), 2020.

[Multi-pretraining for Large-scale Text Classification.](https://www.aclweb.org/anthology/2020.findings-emnlp.185.pdf), EMNLP(Findings), 2020.

[BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA.](https://www.aclweb.org/anthology/2020.findings-emnlp.307.pdf), EMNLP, 2020.

[Document Ranking with a Pretrained Sequence-to-Sequence Model.](https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf), EMNLP(Findings), 2020.

[Probing Pretrained Language Models for Lexical Semantics.](https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf), EMNLP, 2020.

[Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models.](https://www.aclweb.org/anthology/2020.findings-emnlp.25.pdf), EMNLP(Findings), 2020.

[Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models](https://www.aclweb.org/anthology/2020.emnlp-main.679.pdf), EMNLP, 2020

[Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning](https://www.aclweb.org/anthology/2020.findings-emnlp.286.pdf), EMNLP(Findings), 2020.

[A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media](https://www.aclweb.org/anthology/2020.emnlp-main.619.pdf), EMNLP, 2020.

[TNT: Text Normalization based Pre-training of Transformers for Content Moderation](https://www.aclweb.org/anthology/2020.emnlp-main.383.pdf), EMNLP, 2020.

[Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations](https://www.aclweb.org/anthology/2020.emnlp-main.108.pdf), EMNLP, 2020.

[Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation](https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf), EMNLP, 2020.

[Table Fact Verification with Structure-Aware Transformer](https://www.aclweb.org/anthology/2020.emnlp-main.126.pdf), EMNLP, 2020.

[Taming Pretrained Transformers for Extreme Multi-label Text Classification](https://arxiv.org/pdf/1905.02331.pdf), KDD, 2020.

[Reciptor: An Effective Pretrained Model for Recipe Representation Learning](https://dl.acm.org/doi/pdf/10.1145/3394486.3403223), KDD, 2020.

[Pretraining Methods for Dialog Context Representation Learning.](https://www.aclweb.org/anthology/P19-1373.pdf), ACL, 2019.

[Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings.](https://www.aclweb.org/anthology/P19-1168.pdf), ACL, 2019

## Vision-Language PTMs
### Image-Based VL-PTMs

### Video-Based VL-PTMs
[Video-Grounded Dialogues with Pretrained Generation Language Models.](https://www.aclweb.org/anthology/2020.acl-main.518.pdf), ACL, 2020.



### 3D VL-PTMs


