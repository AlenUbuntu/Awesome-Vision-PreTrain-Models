# Awesome Vision and Language PreTrain Models (PTMs)
Maintained by [Yang Gao]() (ustcgaoy01@gmail.com) Last Update on 12/05/2020. 

Due to the large amount of research in this field, we mainly focus on research related to transfer learning, multimodal learning and SOTA modeling techniques for improving model performance or efficiency.

## Table of Contents
* [Surveys](#survey)
* [Transformers](#transformers)
* [Well-Known Large-Scale Pretrain Models](#well-known-large-scale-pretrain-models)
* [Vision-Only PTMs](#vision-only-ptms)
* [Language-Only PTMs](#language-only-ptms)
* [Vision-Language PTMs](#vision-language-ptms)

## Survey
[Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)

[Transformers: State-of-the-Art Natural Language Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6.pdf)

## Transformers
![](https://github.com/AlenUbuntu/Awesome-Vision-and-Language-PreTrain-Models/blob/main/transformers.png)

**Performer**

[Masked language modeling for proteins via linearly scalable long-context transformers](https://arxiv.org/pdf/2006.03555.pdf), arXiv, 2020.

**Linformer**

[Linformer: Selfattention with linear complexity.](https://arxiv.org/pdf/2006.04768.pdf), arXiv, 2020.

**Linear Transformers**

[Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf), arXiv, 2020.

**BigBird**

[Big bird: Transformers for longer sequences.](https://arxiv.org/pdf/2007.14062.pdf), arXiv, 2020.

**Synthesizer**

[Synthesizer: Rethinking self-attention in transformer models.](https://arxiv.org/pdf/2005.00743.pdf), arXiv, 2020.

**ETC**

[Etc: Encoding long and structured data in transformers.](https://arxiv.org/pdf/2004.08483.pdf), arXiv, 2020.

**Longformer**

[Longformer: The long-document transformer.](https://arxiv.org/pdf/2004.05150.pdf), arXiv, 2020.

**Sinkhorn Transformer**

[Sparse sinkhorn attention.](https://arxiv.org/pdf/2002.11296.pdf), arXiv, 2020.

**Compressive Transformer**

[Compressive transformers for long-range sequence modelling.](https://arxiv.org/pdf/1911.05507.pdf), ICLR, 2020.

**Routing Transformer**

[Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/pdf/2003.05997.pdf), arXiv, 2020.

**Reformer**

[Reformer: The efficient transformer.](https://arxiv.org/pdf/2001.04451.pdf), ICLR, 2020.

**Axial Transformer**

[Axial attention in multidimensional transformers.](https://arxiv.org/pdf/1912.12180.pdf), arXiv, 2019.

**Sparse Transformer**

[Generating long sequences with sparse transformers.](https://arxiv.org/pdf/1904.10509.pdf)

**Transformer-XL**

[Transformer-xl: Attentive language models beyond a fixed-length context.](https://arxiv.org/pdf/1901.02860.pdf), arXiv, 2019.

**Set Transformer**

[Set transformer: A framework for attention-based permutation-invariant neural networks.](https://arxiv.org/pdf/1810.00825.pdf), ICML, 2019.

**Image Transformer**

[Image transformer.](https://arxiv.org/pdf/1802.05751.pdf), ICML, 2018.

**Memory Compressed**

[Generating wikipedia by summarizing long sequences](https://arxiv.org/pdf/1801.10198.pdf), ICLR, 2018.

**Transformer**

[Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf), NuerIPS, 2017

**Memory-driven Transformer**

[Generating Radiology Reports via Memory-driven Transformer](https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf), EMNLP, 2020.

**Highway Transformer**

[Highway Transformer: Self-Gating Enhanced Self-Attentive Networks](https://www.aclweb.org/anthology/2020.acl-main.616.pdf), ACL, 2020.

## Well-Known Large-Scale Pretrain Models
---
### General
**Reformer** (General)

[Reformer: The efficient transformer.](https://arxiv.org/pdf/2001.04451.pdf), ICLR, 2020.

---
### Language
**ELECTRA** 

[ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS](https://arxiv.org/pdf/2003.10555.pdf), ICLR, 2020.

**ALBERT** 

[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf), ICLR, 2020.

**Longformer** 

[Longformer: The long-document transformer.](https://arxiv.org/pdf/2004.05150.pdf), arXiv, 2020.

**XLM**

[Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf), NeurIPS, 2019

**DistilBERT**

[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf), NeurIPS, 2019

**T5**

[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf), JMLR, 2019.

**Bart**

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf), ACL, 2019.

**XLNet**

[XLNet: Generalized Autoregressive Pretraining for Language Understanding.](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf), NIPS, 2019.

**Transformer-XL**

[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf), ACL, 2019.

**GPT/GPT2**

[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), OpenAI blog, 2019

**RoBERTa**

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf), arXiv, 2019.

**Bert**

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf), NAACL, 2019.

---
### Vision
---
### Multimodal
**MMBT**

[Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf), arXiv, 2019

**MAG-BERT, MAG-XLNet**

[Integrating Multimodal Information in Large Pretrained Transformers.](https://www.aclweb.org/anthology/2020.acl-main.214.pdf), ACL, 2020.

**TaBERT**

[TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data.](https://www.aclweb.org/anthology/2020.acl-main.745.pdf), ACL, 2020.

**X-LXMERT**

[X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf), EMNLP, 2020.

## Vision-Only PTMs
[Modeling Techniques, Transfer Learning and Applications](https://github.com/AlenUbuntu/Awesome-Vision-and-Language-PreTrain-Models/blob/main/VisionOnlyPTMs.md)

## Language-Only PTMs
[Modeling Techniques, Transfer Learning and Applications](https://github.com/AlenUbuntu/Awesome-Vision-and-Language-PreTrain-Models/blob/main/LanguageOnlyPTMs.md)

## Vision-Language PTMs
[VL-PTMs](https://github.com/AlenUbuntu/Awesome-Vision-and-Language-PreTrain-Models/blob/main/VL-PTMs.md)

